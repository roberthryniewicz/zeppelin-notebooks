{"paragraphs":[{"text":"%md\n\n![sv-image](https://raw.githubusercontent.com/roberthryniewicz/images/master/silicon_valley_corporation.jpg)\n\n## Apache Spark in 5 Minutes \n#### Exploring Silicon Valley Show Episodes Dataset\n\n**Level**: Beginner\n**Language**: Python\n**Requirements**: \n- [HDP 2.6](http://hortonworks.com/products/sandbox/) (or later)\n- Spark 2.x\n\n**Author**: Robert Hryniewicz","user":"admin","dateUpdated":"2018-03-14T00:40:28+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><img src=\"https://raw.githubusercontent.com/roberthryniewicz/images/master/silicon_valley_corporation.jpg\" alt=\"sv-image\" /></p>\n<h2>Apache Spark in 5 Minutes</h2>\n<h4>Exploring Silicon Valley Show Episodes Dataset</h4>\n<p><strong>Level</strong>: Beginner<br/><strong>Language</strong>: Python<br/><strong>Requirements</strong>:<br/>- <a href=\"http://hortonworks.com/products/sandbox/\">HDP 2.6</a> (or later)<br/>- Spark 2.x</p>\n<p><strong>Author</strong>: Robert Hryniewicz</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396849_476048879","id":"20161013-011142_1891215806","dateCreated":"2018-03-13T23:56:36+0000","dateStarted":"2018-03-14T00:40:28+0000","dateFinished":"2018-03-14T00:40:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5559"},{"title":"Short Intro","text":"%md\n\nWelcome to a quick overview of Apache Spark with Sillicon Valley Episodes dataset. If you've never watched the Silicon Valley show you can learn more about it [here](https://en.wikipedia.org/wiki/Silicon_Valley_(TV_series)). \n\nIn this notebook we will download the dataset (in JSON format) from an external github repository, ingest it into a Spark Dataset and perform basic analysis, filtering, and word count.","dateUpdated":"2018-03-13T23:56:36+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Welcome to a quick overview of Apache Spark with Sillicon Valley Episodes dataset. If you&rsquo;ve never watched the Silicon Valley show you can learn more about it [here](<a href=\"https://en.wikipedia.org/wiki/Silicon_Valley_(TV_series))\">https://en.wikipedia.org/wiki/Silicon_Valley_(TV_series))</a>. </p>\n<p>In this notebook we will download the dataset (in JSON format) from an external github repository, ingest it into a Spark Dataset and perform basic analysis, filtering, and word count.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396849_476048879","id":"20161013-011155_1645524279","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5560"},{"title":"New to Zeppelin?","text":"%md\n\nIf you haven't already, checkout the [Hortonworks Apache Zeppelin](https://hortonworks.com/apache/zeppelin/) page as well as the [Getting Started with Apache Zeppelin](http://hortonworks.com/hadoop-tutorial/getting-started-apache-zeppelin/) tutorial.\n\nYou will find the official Apache Zeppelin page [here](https://zeppelin.apache.org/).","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>If you haven&rsquo;t already, checkout the <a href=\"https://hortonworks.com/apache/zeppelin/\">Hortonworks Apache Zeppelin</a> page as well as the <a href=\"http://hortonworks.com/hadoop-tutorial/getting-started-apache-zeppelin/\">Getting Started with Apache Zeppelin</a> tutorial.</p>\n<p>You will find the official Apache Zeppelin page <a href=\"https://zeppelin.apache.org/\">here</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161014-155201_679736099","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5561"},{"title":"New to Spark?","text":"%md\n\nApache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets.\n\nIf you would like to learn more about Apache Spark visit:\n- [Official Apache Spark Page](http://spark.apache.org/)\n- [Hortonworks Apache Spark Page](http://hortonworks.com/apache/spark/)\n- [Hortonworks Apache Spark Docs](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html)","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Apache Spark is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to datasets.</p>\n<p>If you would like to learn more about Apache Spark visit:<br/>- <a href=\"http://spark.apache.org/\">Official Apache Spark Page</a><br/>- <a href=\"http://hortonworks.com/apache/spark/\">Hortonworks Apache Spark Page</a><br/>- <a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html\">Hortonworks Apache Spark Docs</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161014-121442_628671851","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5562"},{"title":"How to run a paragraph?","text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161014-144044_1782842084","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5563"},{"title":"What are Interpreters?","text":"%md\n\nIn the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with `%` followed by an interpreter name, e.g. `%livy.pyspark` is a pyspark Livy interpreter providing a REST inteface to Spark 2.x. Different interpreter names indicate what will be executed: code, markdown, html etc.This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!\n\nThroughtout this notebook we will use the following interpreters:\n\n- `%livy.pyspark` - Livy pyspark interpreter to run Spark 2.x code written in Python\n- `%livy.sql` - Spark SQL interprter (to execute SQL queries against temporary tables in Spark)\n- `%angular` - Angular interpreter to run Angular and HTML code\n- `%md` - Markdown for displaying formatted text, links, and images\n\nTo learn more about Zeppelin interpreters check out this [link](https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html).","user":"admin","dateUpdated":"2018-03-14T00:43:12+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with <code>%</code> followed by an interpreter name, e.g. <code>%livy.pyspark</code> is a pyspark Livy interpreter providing a REST inteface to Spark 2.x. Different interpreter names indicate what will be executed: code, markdown, html etc.This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!</p>\n<p>Throughtout this notebook we will use the following interpreters:</p>\n<ul>\n  <li><code>%livy.pyspark</code> - Livy pyspark interpreter to run Spark 2.x code written in Python</li>\n  <li><code>%livy.sql</code> - Spark SQL interprter (to execute SQL queries against temporary tables in Spark)</li>\n  <li><code>%angular</code> - Angular interpreter to run Angular and HTML code</li>\n  <li><code>%md</code> - Markdown for displaying formatted text, links, and images</li>\n</ul>\n<p>To learn more about Zeppelin interpreters check out this <a href=\"https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html\">link</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161014-145714_450762590","dateCreated":"2018-03-13T23:56:36+0000","dateStarted":"2018-03-14T00:43:12+0000","dateFinished":"2018-03-14T00:43:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5564"},{"title":"Some initial delay to be expected...","text":"%md\n**Note**: The first time you run `spark.version` in the paragraph below, several services will initialize in the background. \nThis may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running.","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Note</strong>: The first time you run <code>spark.version</code> in the paragraph below, several services will initialize in the background.<br/>This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161014-144409_1067974024","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5565"},{"title":"Verify Spark Version (should be 2.x)","text":"%livy.pyspark\n\nspark.version","user":"admin","dateUpdated":"2018-03-14T00:39:10+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"u'2.2.0.2.6.4.0-91'"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161012-235330_1461856587","dateCreated":"2018-03-13T23:56:36+0000","dateStarted":"2018-03-14T00:39:10+0000","dateFinished":"2018-03-14T00:39:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5566"},{"text":"%livy.pyspark\n\nimport urllib2\nfile = urllib2.urlopen(\"https://raw.githubusercontent.com/roberthryniewicz/datasets/master/svepisodes.json\")\nwith open('/tmp/svepisodes.json','wb') as output:\n  output.write(file.read())","user":"admin","dateUpdated":"2018-03-14T00:31:13+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]},"apps":[],"jobName":"paragraph_1520987305471_-93300704","id":"20180314-002825_508469821","dateCreated":"2018-03-14T00:28:25+0000","dateStarted":"2018-03-14T00:31:13+0000","dateFinished":"2018-03-14T00:31:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5567"},{"title":"Load data into a Spark DataFrame","text":"%livy.pyspark\n\npath = \"file:///tmp/svepisodes.json\"\nsvEpisodes = spark.read.json(path)         # Create a DataFrame from JSON data (automatically infer schema and data types)","user":"admin","dateUpdated":"2018-03-14T00:43:49+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161012-200853_1560821654","dateCreated":"2018-03-13T23:56:36+0000","dateStarted":"2018-03-14T00:43:49+0000","dateFinished":"2018-03-14T00:43:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5568"},{"title":"What are Datasets and DataFrames?","text":"%md\n\n**Datasets** and **DataFrames** are distributed collections of data created from a variety of sources: JSON and XML files, tables in Hive, external databases and more. Conceptually, they are equivalent to a table in a relational database or a DataFrame in R or Python. Key difference between the  Dataset and the DataFrame is that Datasets are strongly typed.\n\nThere are complex manipulations possible on Datasets and DataFrames, however they are beyond this quick guide.\n\nTo learn more about Datasets and DataFrames checkout this  [link](http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#datasets-and-dataframes).","dateUpdated":"2018-03-13T23:56:36+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Datasets</strong> and <strong>DataFrames</strong> are distributed collections of data created from a variety of sources: JSON and XML files, tables in Hive, external databases and more. Conceptually, they are equivalent to a table in a relational database or a DataFrame in R or Python. Key difference between the Dataset and the DataFrame is that Datasets are strongly typed.</p>\n<p>There are complex manipulations possible on Datasets and DataFrames, however they are beyond this quick guide.</p>\n<p>To learn more about Datasets and DataFrames checkout this <a href=\"http://spark.apache.org/docs/2.0.0/sql-programming-guide.html#datasets-and-dataframes\">link</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396850_477203125","id":"20161014-131031_180366265","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5569"},{"title":"Print DataFrame Schema","text":"%livy.pyspark\n\nsvEpisodes.printSchema()","user":"admin","dateUpdated":"2018-03-14T00:44:06+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- airdate: string (nullable = true)\n |-- airstamp: string (nullable = true)\n |-- airtime: string (nullable = true)\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- number: long (nullable = true)\n |-- runtime: long (nullable = true)\n |-- season: long (nullable = true)\n |-- summary: string (nullable = true)\n |-- url: string (nullable = true)"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161012-202011_596248668","dateCreated":"2018-03-13T23:56:36+0000","dateStarted":"2018-03-14T00:44:06+0000","dateFinished":"2018-03-14T00:44:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5570"},{"title":"Data Description","text":"%angular\n\n<!DOCTYPE html>\n<html>\n<head>\n<style>\ntable {\n    font-family: arial, sans-serif;\n    border-collapse: collapse;\n    width: 70%;\n}\n\ntd, th {\n    border: 1px solid #dddddd;\n    text-align: left;\n    padding: 8px;\n}\n\ntr:nth-child(even) {\n    background-color: #dddddd;\n}\n</style>\n</head>\n\n<table>\n<tbody>\n<tr>\n  <th></th>\n  <th>Column Name</th>\n  <th>Description</th>\n</tr><tr>\n  <td>1</td>\n  <td>Airdate</td>\n  <td>Date when an episode was aired</td>\n</tr><tr>\n  <td>2</td>\n  <td>Airstamp</td>\n  <td>Timestamp when an episode was aired</td>\n</tr><tr>\n  <td>3</td>\n  <td>Airtime</td>\n  <td>Length of an actual episode airtime (no commercials)</td>\n</tr><tr>\n  <td>4</td>\n  <td>Id</td>\n  <td>Unique show id</td>\n</tr><tr>\n  <td>5</td>\n  <td>Name</td>\n  <td>Name of an episode </td>\n</tr><tr>\n  <td>6</td>\n  <td>Number</td>\n  <td>Episode number</td>\n</tr><tr>\n  <td>7</td>\n  <td>Runtime</td>\n  <td>Total length of an episode (including commercials)</td>\n</tr><tr>\n  <td>8</td>\n  <td>Season</td>\n  <td>Show season</td>\n</tr><tr>\n  <td>9</td>\n  <td>Summary</td>\n  <td>Brief summary of an episode</td>\n</tr><tr>\n  <td>10</td>\n  <td>Url</td>\n  <td>Url where more information is available online about an episode</td>\n</tr>\n</tbody></table>\n\n</body>\n</html>\n","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<!DOCTYPE html>\n<html>\n<head>\n<style>\ntable {\n    font-family: arial, sans-serif;\n    border-collapse: collapse;\n    width: 70%;\n}\n\ntd, th {\n    border: 1px solid #dddddd;\n    text-align: left;\n    padding: 8px;\n}\n\ntr:nth-child(even) {\n    background-color: #dddddd;\n}\n</style>\n</head>\n\n<table>\n<tbody>\n<tr>\n  <th></th>\n  <th>Column Name</th>\n  <th>Description</th>\n</tr><tr>\n  <td>1</td>\n  <td>Airdate</td>\n  <td>Date when an episode was aired</td>\n</tr><tr>\n  <td>2</td>\n  <td>Airstamp</td>\n  <td>Timestamp when an episode was aired</td>\n</tr><tr>\n  <td>3</td>\n  <td>Airtime</td>\n  <td>Length of an actual episode airtime (no commercials)</td>\n</tr><tr>\n  <td>4</td>\n  <td>Id</td>\n  <td>Unique show id</td>\n</tr><tr>\n  <td>5</td>\n  <td>Name</td>\n  <td>Name of an episode </td>\n</tr><tr>\n  <td>6</td>\n  <td>Number</td>\n  <td>Episode number</td>\n</tr><tr>\n  <td>7</td>\n  <td>Runtime</td>\n  <td>Total length of an episode (including commercials)</td>\n</tr><tr>\n  <td>8</td>\n  <td>Season</td>\n  <td>Show season</td>\n</tr><tr>\n  <td>9</td>\n  <td>Summary</td>\n  <td>Brief summary of an episode</td>\n</tr><tr>\n  <td>10</td>\n  <td>Url</td>\n  <td>Url where more information is available online about an episode</td>\n</tr>\n</tbody></table>\n\n</body>\n</html>"}]},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161014-140056_345247395","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5571"},{"title":"Show DataFrame Contents","text":"%livy.pyspark\n\nsvEpisodes.show()","dateUpdated":"2018-03-14T00:44:41+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161012-234401_1548074862","dateCreated":"2018-03-13T23:56:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5572","user":"admin","dateFinished":"2018-03-14T00:44:43+0000","dateStarted":"2018-03-14T00:44:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+--------------------+-------+------+--------------------+------+-------+------+--------------------+--------------------+\n|   airdate|            airstamp|airtime|    id|                name|number|runtime|season|             summary|                 url|\n+----------+--------------------+-------+------+--------------------+------+-------+------+--------------------+--------------------+\n|2014-04-06|2014-04-06T22:00:...|  22:00| 10897|Minimum Viable Pr...|     1|     30|     1|Attending an elab...|http://www.tvmaze...|\n|2014-04-13|2014-04-13T22:00:...|  22:00| 10898|       The Cap Table|     2|     30|     1|After a celebrato...|http://www.tvmaze...|\n|2014-04-20|2014-04-20T22:00:...|  22:00| 10899|Articles of Incor...|     3|     30|     1|While Gavin Belso...|http://www.tvmaze...|\n|2014-04-27|2014-04-27T22:00:...|  22:00| 10900|    Fiduciary Duties|     4|     30|     1|At Peter's toga p...|http://www.tvmaze...|\n|2014-05-04|2014-05-04T22:00:...|  22:00| 10901|      Signaling Risk|     5|     30|     1|Erlich  convinces...|http://www.tvmaze...|\n|2014-05-11|2014-05-11T22:00:...|  22:00| 10902|Third Party Insou...|     6|     30|     1|Richard feels thr...|http://www.tvmaze...|\n|2014-05-18|2014-05-18T22:00:...|  22:00| 10903|    Proof of Concept|     7|     30|     1|At TechCrunch Dis...|http://www.tvmaze...|\n|2014-06-01|2014-06-01T22:00:...|  22:00| 10904|Optimal Tip-to-Ti...|     8|     30|     1|Poised to compete...|http://www.tvmaze...|\n|2015-04-12|2015-04-12T22:00:...|  22:00|117409|   Sand Hill Shuffle|     1|     30|     2|Season 2 begins w...|http://www.tvmaze...|\n|2015-04-19|2015-04-19T22:00:...|  22:00|142992| Runaway Devaluation|     2|     30|     2|Pied Piper could ...|http://www.tvmaze...|\n|2015-04-26|2015-04-26T22:00:...|  22:00|142993|           Bad Money|     3|     30|     2|Richard mulls a p...|http://www.tvmaze...|\n|2015-05-03|2015-05-03T22:00:...|  22:00|142994|            The Lady|     4|     30|     2|Richard butts hea...|http://www.tvmaze...|\n|2015-05-10|2015-05-10T22:00:...|  22:00|153965|        Server Space|     5|     30|     2|Gavin creates int...|http://www.tvmaze...|\n|2015-05-17|2015-05-17T22:00:...|  22:00|154580|            Homicide|     6|     30|     2|Erlich runs into ...|http://www.tvmaze...|\n|2015-05-24|2015-05-24T22:00:...|  22:00|155129|       Adult Content|     7|     30|     2|The team fields j...|http://www.tvmaze...|\n|2015-05-31|2015-05-31T22:00:...|  22:00|155130| White Hat/Black Hat|     8|     30|     2|Richard gets para...|http://www.tvmaze...|\n|2015-06-07|2015-06-07T22:00:...|  22:00|155199| Binding Arbitration|     9|     30|     2|Erlich wants to t...|http://www.tvmaze...|\n|2015-06-14|2015-06-14T22:00:...|  22:00|155200|Two Days of The C...|    10|     30|     2|As the guys await...|http://www.tvmaze...|\n|2016-04-24|2016-04-24T22:00:...|  22:00|560883|    Founder Friendly|     1|     30|     3|After being uncer...|http://www.tvmaze...|\n|2016-05-01|2016-05-01T22:00:...|  22:00|668661|      Two in the Box|     2|     30|     3|The new and impro...|http://www.tvmaze...|\n+----------+--------------------+-------+------+--------------------+------+-------+------+--------------------+--------------------+\nonly showing top 20 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]}},{"title":"Is there a more interactive way to display query results?","text":"%md\n\nShort answer, yes! The data displayed in the paragraph above isn't too interactive. To have a more dynamic experience, let's create a temporary (in-memory) view that we can query against and interact with the resulting data in a table or graph format. The temporary view will allow us to run SQL queries to get back results.\n\nNote that the temporary view will reside in memory as long as the Spark session is alive.","dateUpdated":"2018-03-13T23:56:36+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Short answer, yes! The data displayed in the paragraph above isn&rsquo;t too interactive. To have a more dynamic experience, let&rsquo;s create a temporary (in-memory) view that we can query against and interact with the resulting data in a table or graph format. The temporary view will allow us to run SQL queries to get back results.</p>\n<p>Note that the temporary view will reside in memory as long as the Spark session is alive.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161013-005846_439497469","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5573"},{"title":"Create a Temporary View","text":"%livy.pyspark\n\n# Creates a temporary view\nsvEpisodes.createOrReplaceTempView(\"svepisodes\")","dateUpdated":"2018-03-14T00:46:09+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":[],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161012-202125_3295223","dateCreated":"2018-03-13T23:56:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5574","user":"admin","dateFinished":"2018-03-14T00:46:10+0000","dateStarted":"2018-03-14T00:46:09+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]}},{"title":"So now what?","text":"%md\n\nAt this point we can run queries using a familiar SQL syntax against our newly registered `svepisodes` table. \n\nNote that although we are using a SQL syntax in the following paragraph it is translated and executed using the Spark engine with all the expected optimizations.","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>At this point we can run queries using a familiar SQL syntax against our newly registered <code>svepisodes</code> table. </p>\n<p>Note that although we are using a SQL syntax in the following paragraph it is translated and executed using the Spark engine with all the expected optimizations.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161013-182547_1601163342","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5575"},{"title":"View Data in an Interactive Table Format","text":"%livy.sql\n\nSELECT * FROM svepisodes ORDER BY season, number","user":"admin","dateUpdated":"2018-03-14T00:46:16+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"airdate","index":0,"aggr":"sum"}],"values":[{"name":"airstamp","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"airdate","index":0,"aggr":"sum"},"yAxis":{"name":"airstamp","index":1,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"airdate\tairstamp\tairtime\tid\tname\tnumber\truntime\tseason\tsummary\turl\n2014-04-06\t2014-04-06T22:00:...\t22:00\t10897\tMinimum Viable Pr...\t1\t30\t1\tAttending an elab...\thttp://www.tvmaze...\n2014-04-13\t2014-04-13T22:00:...\t22:00\t10898\tThe Cap Table\t2\t30\t1\tAfter a celebrato...\thttp://www.tvmaze...\n2014-04-20\t2014-04-20T22:00:...\t22:00\t10899\tArticles of Incor...\t3\t30\t1\tWhile Gavin Belso...\thttp://www.tvmaze...\n2014-04-27\t2014-04-27T22:00:...\t22:00\t10900\tFiduciary Duties\t4\t30\t1\tAt Peter's toga p...\thttp://www.tvmaze...\n2014-05-04\t2014-05-04T22:00:...\t22:00\t10901\tSignaling Risk\t5\t30\t1\tErlich  convinces...\thttp://www.tvmaze...\n2014-05-11\t2014-05-11T22:00:...\t22:00\t10902\tThird Party Insou...\t6\t30\t1\tRichard feels thr...\thttp://www.tvmaze...\n2014-05-18\t2014-05-18T22:00:...\t22:00\t10903\tProof of Concept\t7\t30\t1\tAt TechCrunch Dis...\thttp://www.tvmaze...\n2014-06-01\t2014-06-01T22:00:...\t22:00\t10904\tOptimal Tip-to-Ti...\t8\t30\t1\tPoised to compete...\thttp://www.tvmaze...\n2015-04-12\t2015-04-12T22:00:...\t22:00\t117409\tSand Hill Shuffle\t1\t30\t2\tSeason 2 begins w...\thttp://www.tvmaze...\n2015-04-19\t2015-04-19T22:00:...\t22:00\t142992\tRunaway Devaluation\t2\t30\t2\tPied Piper could ...\thttp://www.tvmaze...\n2015-04-26\t2015-04-26T22:00:...\t22:00\t142993\tBad Money\t3\t30\t2\tRichard mulls a p...\thttp://www.tvmaze...\n2015-05-03\t2015-05-03T22:00:...\t22:00\t142994\tThe Lady\t4\t30\t2\tRichard butts hea...\thttp://www.tvmaze...\n2015-05-10\t2015-05-10T22:00:...\t22:00\t153965\tServer Space\t5\t30\t2\tGavin creates int...\thttp://www.tvmaze...\n2015-05-17\t2015-05-17T22:00:...\t22:00\t154580\tHomicide\t6\t30\t2\tErlich runs into ...\thttp://www.tvmaze...\n2015-05-24\t2015-05-24T22:00:...\t22:00\t155129\tAdult Content\t7\t30\t2\tThe team fields j...\thttp://www.tvmaze...\n2015-05-31\t2015-05-31T22:00:...\t22:00\t155130\tWhite Hat/Black Hat\t8\t30\t2\tRichard gets para...\thttp://www.tvmaze...\n2015-06-07\t2015-06-07T22:00:...\t22:00\t155199\tBinding Arbitration\t9\t30\t2\tErlich wants to t...\thttp://www.tvmaze...\n2015-06-14\t2015-06-14T22:00:...\t22:00\t155200\tTwo Days of The C...\t10\t30\t2\tAs the guys await...\thttp://www.tvmaze...\n2016-04-24\t2016-04-24T22:00:...\t22:00\t560883\tFounder Friendly\t1\t30\t3\tAfter being uncer...\thttp://www.tvmaze...\n2016-05-01\t2016-05-01T22:00:...\t22:00\t668661\tTwo in the Box\t2\t30\t3\tThe new and impro...\thttp://www.tvmaze...\n2016-05-08\t2016-05-08T22:00:...\t22:00\t668662\tMeinertzhagen's H...\t3\t30\t3\tRichard searches ...\thttp://www.tvmaze...\n2016-05-15\t2016-05-15T22:00:...\t22:00\t670680\tMaleant Data Syst...\t4\t30\t3\tThe Pied Piper gu...\thttp://www.tvmaze...\n2016-05-22\t2016-05-22T22:00:...\t22:00\t670682\tThe Empty Chair\t5\t30\t3\tRichard lets his ...\thttp://www.tvmaze...\n2016-05-29\t2016-05-29T22:00:...\t22:00\t670681\tBachmanity Insanity\t6\t30\t3\tRichard's new rel...\thttp://www.tvmaze...\n2016-06-05\t2016-06-05T22:00:...\t22:00\t717453\tTo Build a Better...\t7\t30\t3\tWhen the guys dec...\thttp://www.tvmaze...\n2016-06-12\t2016-06-12T22:00:...\t22:00\t729570\tBachman's Earning...\t8\t30\t3\tErlich struggles ...\thttp://www.tvmaze...\n2016-06-19\t2016-06-19T22:00:...\t22:00\t729571\tDaily Active Users\t9\t30\t3\tUpon discovering ...\thttp://www.tvmaze...\n2016-06-26\t2016-06-26T22:00:...\t22:00\t729572\tThe Uptick\t10\t30\t3\tWith Pied Piper's...\thttp://www.tvmaze..."},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161013-005646_1818766386","dateCreated":"2018-03-13T23:56:36+0000","dateStarted":"2018-03-14T00:46:16+0000","dateFinished":"2018-03-14T00:46:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5576"},{"title":"Can we do something useful?","text":"%md\n\nOK, so now let's run a slightly more complex SQL query on the underlying table data.","dateUpdated":"2018-03-14T01:16:40+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>OK, so now let&rsquo;s run a slightly more complex SQL query on the underlying table data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161013-182951_885833546","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5577"},{"title":"Total Number of Episodes","text":"%livy.sql\n\nSELECT count(1) AS TotalNumEpisodes FROM svepisodes","dateUpdated":"2018-03-14T00:46:35+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql"},"colWidth":4,"editorMode":"ace/mode/sql","title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"TotalNumEpisodes","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"TotalNumEpisodes","index":0,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161017-235756_1441150850","dateCreated":"2018-03-13T23:56:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5578","user":"admin","dateFinished":"2018-03-14T00:46:37+0000","dateStarted":"2018-03-14T00:46:35+0000","results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"TotalNumEpisodes\n28"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]}},{"title":"Number of Episodes per Season","text":"%livy.sql \n\nSELECT season, count(number) as episodes FROM svepisodes GROUP BY season","dateUpdated":"2018-03-14T00:46:42+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"sql"},"colWidth":8,"editorMode":"ace/mode/sql","editorHide":false,"title":true,"results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"season","index":0,"aggr":"sum"}],"values":[{"name":"episodes","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"season","index":0,"aggr":"sum"}}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520985396851_476818376","id":"20161012-202204_1707933023","dateCreated":"2018-03-13T23:56:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5579","user":"admin","dateFinished":"2018-03-14T00:46:48+0000","dateStarted":"2018-03-14T00:46:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"season\tepisodes\n1\t8\n3\t10\n2\t10"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]}},{"title":"Word Count on Episode Summaries","text":"%md\n\nNow let's perform a basic word-count on the summary column and find out which words occur most frequently. This should give us some indication on the popularity of certain characters and other relevant keywords in the context of the Sillicon Valley show.","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now let&rsquo;s perform a basic word-count on the summary column and find out which words occur most frequently. This should give us some indication on the popularity of certain characters and other relevant keywords in the context of the Sillicon Valley show.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396854_475664130","id":"20161013-010351_1570854534","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5580"},{"title":"Raw Word Count","text":"%livy.pyspark\n\n# select summaries & conver the DataFrame into a Resilient Distributed Dataset (RDD)\nsvSummaries = svEpisodes.select(\"summary\").rdd\n\n# split on flatten text, select words (split on whitespace), remove empty words, lowercase\nwords = svSummaries.flatMap(lambda text: text).flatMap(lambda line: line.split(\" \")).filter(lambda word: word != \"\").map(lambda word: word.lower())\n\n# preview result\nwords.take(10)","dateUpdated":"2018-03-14T02:00:59+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520985396854_475664130","id":"20161013-000142_472015281","dateCreated":"2018-03-13T23:56:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5581","user":"admin","dateFinished":"2018-03-14T02:01:00+0000","dateStarted":"2018-03-14T02:00:59+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[u'attending', u'an', u'elaborate', u'launch', u'party,', u'richard', u'and', u'his', u'computer', u'programmer']"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]}},{"text":"%livy.pyspark\n\n# create word-number pairs\nwordNum = words.map(lambda word: (word, 1))\n\n# use reduce function to count number of words\nwordCount = wordNum.reduceByKey(lambda a, b: a + b)\n\n# return words with count > 10\nresult = wordFilteredCount.filter(lambda (word, count): count > 10).collect()\n\nprint result","user":"admin","dateUpdated":"2018-03-14T02:16:11+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520988541638_1213513997","id":"20180314-004901_1605242440","dateCreated":"2018-03-14T00:49:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:8148","dateFinished":"2018-03-14T02:15:06+0000","dateStarted":"2018-03-14T02:15:05+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[(u'erlich', 16), (u'richard', 26), (u'piper', 12), (u'jared', 11), (u'his', 16), (u'big', 11), (u'pied', 24), (u'gavin', 14), (u'dinesh', 16)]"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1520983684154_0001<br/>Spark WebUI: <a href=\"http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/\">http://sandbox-hdp.hortonworks.com:8088/proxy/application_1520983684154_0001/</a>"}]}},{"title":"Note on the Results","text":"%md\n\nLooks like Richard, Pied Piper, Dinesh, Erlich, Gavin and Jared are the key words in the Sillicon Valley show. Looks like a lot revolves around Richard!","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Looks like Richard, Pied Piper, Dinesh, Erlich, Gavin and Jared are the key words in the Sillicon Valley show. Looks like a lot revolves around Richard!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396854_475664130","id":"20161014-142139_512800114","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5585"},{"title":"Final Comments on Word Count","text":"%md\n\nAs you can see, there's more to do with our word list, e.g. Piper and Piper's should be counted as the same word. There's more, of course, however this is beyond the scope of this quick intro to Apache Spark.","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>As you can see, there&rsquo;s more to do with our word list, e.g. Piper and Piper&rsquo;s should be counted as the same word. There&rsquo;s more, of course, however this is beyond the scope of this quick intro to Apache Spark.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396855_475279381","id":"20161013-010754_1315051750","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5586"},{"title":"Additional Resources","text":"%md\n\nWe hope you've enjoyed this brief intro to Apache Spark. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_zeppelin-component-guide/content/ch_using_zeppelin.html) - official Zeppelin documentation.\n","dateUpdated":"2018-03-13T23:56:36+0000","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":10,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We hope you&rsquo;ve enjoyed this brief intro to Apache Spark. Below are additional resources that you should find useful:</p>\n<ol>\n  <li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n  <li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n  <li><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n  <li><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_zeppelin-component-guide/content/ch_using_zeppelin.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1520985396855_475279381","id":"20160226-200649_425588199","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5587"},{"text":"%angular\n</br>\n<center>\n<a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\" target='_blank'>\n  <img src=\"http://hortonworks.com/wp-content/uploads/2016/03/logo-hcc.png\" alt=\"HCC\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>","dateUpdated":"2018-03-13T23:56:36+0000","config":{"tableHide":false,"editorSetting":{},"colWidth":2,"editorMode":"ace/mode/scala","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"</br>\n<center>\n<a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\" target='_blank'>\n  <img src=\"http://hortonworks.com/wp-content/uploads/2016/03/logo-hcc.png\" alt=\"HCC\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>"}]},"apps":[],"jobName":"paragraph_1520985396855_475279381","id":"20161013-185141_1487979052","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5588"},{"text":"","dateUpdated":"2018-03-13T23:56:36+0000","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520985396855_475279381","id":"20161018-143930_1545375880","dateCreated":"2018-03-13T23:56:36+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5589"}],"name":"Spark in 5 min - Updates with Pyspark - v1a","id":"2D7VR92Q8","angularObjects":{"2CYSFF4KX:shared_process":[],"2CZ7M5Z1U:shared_process":[],"2CZN4E536:shared_process":[],"2CXMXB6B9:shared_process":[],"2CX1UN2AM:shared_process":[],"2CVYP86A3:shared_process":[],"2CZPMT4AR:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}