{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Analyzing Internet of Things Data with IBM DSX: Trucking Data Analysis.\n",
    "\n",
    "In this notebook you will see how to build a predictive model with Spark machine learning API (SparkML) and deploy it for scoring in Machine Learning (ML) in IBM DSX platform.\n",
    "This notebook walks you through following steps:\n",
    "- Fetching data from HDFS\n",
    "- Feature engineering\n",
    "- Data Visualization\n",
    "- Build a binary classifier model with SparkML API\n",
    "- Run basic model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Use Case\n",
    "\n",
    "Imagine a trucking company that dispatches trucks across the country. The trucks are outfitted with sensors that collect data – data like location of the driver, weather conditions, and even what event recently occured (speeding, the truck weaving out of its lane, following too closely, etc). Data like this is generated very often, say once per second and is streamed back to the company’s servers.\n",
    "\n",
    "The company needs a way to process this stream of data and run some analysis on the data so that it can make sure trucks are traveling safe and if the driver is likely to make any violations anytime soon. Oh, and this also needs to be done in real-time!\n",
    "\n",
    "\n",
    "![CRISP-DM](https://raw.githubusercontent.com/dhananjaymehta/IoTtrucking/master/trucks2.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For predicting violations, we are simulating trucking events in terms of location, miles driven, weather conditions. Next step is to visually understand the data and correlations between different features. We will also need to do some feature engineering for data preparation. \n",
    "\n",
    "Once the data is ready, we can build a predictive model. In our example we are using the SparkML Random Forrest classification model. Classification is a statistical technique which assigns a \"class\" to each driver - **\"Violations\"** or **\"Normal\"**. We build the classification models using historical data to train our model. (In a typical analytics project large training datasets will be used but we are building this demo model with a small datasets)\n",
    "\n",
    "If a model's meets accuracy expectations, it is good to be deployed for scoring. \n",
    "\n",
    "Scoring is the process of applying the model to a new set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "#sc=SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Import HDFS Data from remote HDP cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# view dataset\n",
    "!wget -i -L \"https://raw.githubusercontent.com/roberthryniewicz/datasets/master/IoT-Trucking-Demo.csv\" | tail -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Load Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Data : from HDFS\n",
    "#DSX_PROJECT_DIR='/user-home/1002/DSX_Projects/Final DSX Test - Shared/datasets/'\n",
    "eventsFile = SQLContext(sc).read.csv('./IoT-Trucking-Demo.csv', header='true', inferSchema = 'false')  # this will load it as Spark DataFrame\n",
    "# see the data\n",
    "eventsFile.show(5)\n",
    "# total number of records\n",
    "tot_row = eventsFile.count()\n",
    "\n",
    "# events with violations\n",
    "tot_violations = eventsFile.filter(\"iscertified == 'N'\").count()\n",
    "tot_no_violations = tot_row - tot_violations\n",
    "print(type(eventsFile), tot_row)\n",
    "print(\"Violations: \" + str(tot_violations) + \"; No violations: \" + str(tot_no_violations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "eventsFile.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# old column names\n",
    "old_col_names = eventsFile.columns\n",
    "# new names to be assigned\n",
    "new_col_names =['eventtyp', 'iscertified', 'paymentscheme', 'hoursdriven', 'milesdriven', 'latitude', 'longitude', 'isfoggy', 'israiny', 'iswindy']\n",
    "\n",
    "# Renaming the columns\n",
    "eventsdata = reduce(lambda eventsFile, idx: eventsFile.withColumnRenamed(old_col_names[idx], new_col_names[idx]), range(len(old_col_names)), eventsFile)\n",
    "eventsdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Type conversion for Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data=eventsdata.withColumn(\"latitude\", eventsdata[\"latitude\"].cast(\"float\")).withColumn(\"longitude\", eventsdata[\"longitude\"].cast(\"float\")).withColumn(\"hoursdriven\", eventsdata[\"hoursdriven\"].cast(\"int\")).withColumn(\"isfoggy\", eventsdata[\"isfoggy\"].cast(\"int\")).withColumn(\"israiny\", eventsdata[\"israiny\"].cast(\"int\")).withColumn(\"iswindy\", eventsdata[\"iswindy\"].cast(\"int\")).withColumn(\"milesdriven\", eventsdata[\"milesdriven\"].cast(\"int\"))\n",
    "\n",
    "# view final schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "** Transforming truck events** eventType\n",
    "\n",
    "eventType into binary (Y/N) ifViolated\n",
    "\n",
    "**N** - if driving is 'Normal' and there are no violations\n",
    "\n",
    "**Y** - ['Lane Departure', 'Overspeed','Unsafe following distance', 'Unsafe tail distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# creating a pandas dataframe \n",
    "data_pandas=data.toPandas()\n",
    "type(data_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# unique trucking events\n",
    "truck_events= list(data_pandas['eventtyp'].unique())\n",
    "truck_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform column eventType\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "name = 'eventtyp'\n",
    "udf = UserDefinedFunction(lambda x: 'N' if x==\"Normal\" else 'Y', StringType())\n",
    "data_tran=data.select(*[udf(column).alias(name) if column == name else column for column in data.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_pandas=data_tran.toPandas()  # use updated dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Register table for Enriched Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.registerTempTable(\"enrichedEvents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# set plot size\n",
    "fig_size=[0,0]\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 9\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# setting temp dataframe =\n",
    "df = data_pandas\n",
    "\n",
    "# setting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Feature transformation in pandas\n",
    "\n",
    "**Note:** This is for visualization purpose only\n",
    "\n",
    "Setting int values for column\n",
    "\n",
    "- eventTyp:\n",
    "    - 1 if Violation \n",
    "    - 0 for Normal\n",
    "\n",
    "- isCertified: \n",
    "    - 1 if Certified \n",
    "    - 0 for Not Certified\n",
    "\n",
    "- paymentScheme: \n",
    "    - 1 if \"hours\" \n",
    "    - 0 for \"miles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df[\"eventtyp\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['eventtyp'] = df['eventtyp'].apply(lambda x: 0 if x=='N' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df['iscertified'] = df['iscertified'].apply(lambda x: 0 if x=='N' else 1)\n",
    "df['paymentscheme'] = df['paymentscheme'].apply(lambda x: 0 if x=='miles' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**listing the columns**\n",
    "\n",
    "['eventTyp',\n",
    " 'isCertified',\n",
    " 'paymentScheme',\n",
    " 'hoursDriven',\n",
    " 'milesDriven',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'isFoggy',\n",
    " 'isRainy',\n",
    " 'isWindy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Correlation Matrix for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr,cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .7})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "e.g. more miles driven => more seasoned drivers => negative correlation with violations taking place (i.e. eventtyp = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Visualizing multidimensional relationships\n",
    "\n",
    "*exploring correlations between multidimensional data, when you'd like to plot all pairs of values against each other.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='eventtyp', size=2.5);\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Do certified drivers have less violations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Bar Plot\n",
    "sns.regplot(x=\"eventtyp\", y=\"iscertified\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(y=\"eventtyp\", x=\"iscertified\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Correleation btw hours driven and violations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"hoursdriven\", y=\"eventtyp\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What are median hours driven by a driver?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df[\"hoursdriven\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What are median miles driven by a driver?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df[\"milesdriven\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: Building a classifier to predict truck event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# verify data schema\n",
    "data_tran.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_tran.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Algorithm Used: RandomForest Classifier\n",
    "\n",
    "Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are using **ML Pipelines** provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipeline\n",
    "\n",
    "A **Pipeline** is specified as a sequence of stages, and each stage is either a **Transformer** or an **Estimator**. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. \n",
    "\n",
    "For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer’s transform() method is called on the DataFrame.\n",
    "\n",
    "![CRISP-DM](https://raw.githubusercontent.com/dhananjaymehta/IoTtrucking/master/fit.png)\n",
    "\n",
    "For Transformer stages, the transform() method is called on the DataFrame. \n",
    "\n",
    "![CRISP-DM](https://raw.githubusercontent.com/dhananjaymehta/IoTtrucking/master/transform.png)\n",
    "\n",
    "\n",
    "\n",
    "For more details ref: https://spark.apache.org/docs/2.1.1/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "# StringIndexer encodes a string column of labels to a column of label indices\n",
    "\n",
    "SI1 = StringIndexer(inputCol='iscertified',outputCol='iscertifiedEncoded')\n",
    "SI2 = StringIndexer(inputCol='paymentscheme',outputCol='paymentschemeEncoded')\n",
    "\n",
    "#encode the Label column\n",
    "labelIndexer = StringIndexer(inputCol='eventtyp', outputCol='label').fit(data_tran)\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"iscertifiedEncoded\", \"paymentschemeEncoded\", \"hoursdriven\", \"milesdriven\", \"latitude\", \\\n",
    "                                       \"longitude\", \"isfoggy\", \"israiny\", \"iswindy\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)\n",
    "\n",
    "pipeline = Pipeline(stages=[SI1,SI2,labelIndexer, assembler, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = data_tran.randomSplit([0.8,0.2], seed=6)\n",
    "train.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build model. \n",
    "# The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Score test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Showing the prediction results of binary classifier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "results=results.select(results[\"eventtyp\"],results[\"label\"],results[\"predictedlabel\"],results[\"prediction\"],results[\"probability\"])\n",
    "results.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print ('Model accuracy = {:.2f}'.format(results.filter(results.label == results.prediction).count() / float(results.count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TP = (results.filter(results.label == results.prediction).filter(results.prediction == 1.0)).count() # True positive => predicted violation and it did occur\n",
    "TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "FP = (results.filter(results.label != results.prediction).filter(results.prediction == 1.0)).count() # False positive => predicted violation that did not occur\n",
    "FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "precision = float(TP) / (TP + FP)\n",
    "print \"Model precision = {:.2f}\".format(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print 'Area under ROC curve = {:.2f}'.format(evaluator.evaluate(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Area Under ROC Curve\n",
    "\n",
    "The area under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two groups: violation vs no violation.\n",
    "In a ROC curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (Specifity). See https://www.medcalc.org/manual/roc-curves.php\n",
    "\n",
    "#### Evaluation Criteria\n",
    "\n",
    "- .90-1 = excellent (A)\n",
    "- .80-.90 = good (B)\n",
    "- .70-.80 = fair (C)\n",
    "- .60-.70 = poor (D)\n",
    "- .50-.60 = fail (F)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
